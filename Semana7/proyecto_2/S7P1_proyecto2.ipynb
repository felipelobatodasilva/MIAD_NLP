{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto 2 - Clasificación de género de películas\n",
    "\n",
    "El propósito de este proyecto es que puedan poner en práctica, en sus respectivos grupos de trabajo, sus conocimientos sobre técnicas de preprocesamiento, modelos predictivos de NLP, y la disponibilización de modelos. Para su desarrollo tengan en cuenta las instrucciones dadas en la \"Guía del proyecto 2: Clasificación de género de películas\"\n",
    "\n",
    "**Entrega**: La entrega del proyecto deberán realizarla durante la semana 8. Sin embargo, es importante que avancen en la semana 7 en el modelado del problema y en parte del informe, tal y como se les indicó en la guía.\n",
    "\n",
    "Para hacer la entrega, deberán adjuntar el informe autocontenido en PDF a la actividad de entrega del proyecto que encontrarán en la semana 8, y subir el archivo de predicciones a la [competencia de Kaggle](https://www.kaggle.com/t/2c54d005f76747fe83f77fbf8b3ec232)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos para la predicción de género en películas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/moviegenre.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto se usará un conjunto de datos de géneros de películas. Cada observación contiene el título de una película, su año de lanzamiento, la sinopsis o plot de la película (resumen de la trama) y los géneros a los que pertenece (una película puede pertenercer a más de un género). Por ejemplo:\n",
    "- Título: 'How to Be a Serial Killer'\n",
    "- Plot: 'A serial killer decides to teach the secrets of his satisfying career to a video store clerk.'\n",
    "- Generos: 'Comedy', 'Crime', 'Horror'\n",
    "\n",
    "La idea es que usen estos datos para predecir la probabilidad de que una película pertenezca, dada la sinopsis, a cada uno de los géneros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELADO:\n",
    " En el notebook, realicen el preprocesamiento de los datos que consideren pertinente, así como la selección, la calibración y el entrenamiento del modelo predictivo con los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>plot</th>\n",
       "      <th>genres</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3107</th>\n",
       "      <td>2003</td>\n",
       "      <td>Most</td>\n",
       "      <td>most is the story of a single father who takes...</td>\n",
       "      <td>['Short', 'Drama']</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>2008</td>\n",
       "      <td>How to Be a Serial Killer</td>\n",
       "      <td>a serial killer decides to teach the secrets o...</td>\n",
       "      <td>['Comedy', 'Crime', 'Horror']</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6724</th>\n",
       "      <td>1941</td>\n",
       "      <td>A Woman's Face</td>\n",
       "      <td>in sweden ,  a female blackmailer with a disfi...</td>\n",
       "      <td>['Drama', 'Film-Noir', 'Thriller']</td>\n",
       "      <td>7.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4704</th>\n",
       "      <td>1954</td>\n",
       "      <td>Executive Suite</td>\n",
       "      <td>in a friday afternoon in new york ,  the presi...</td>\n",
       "      <td>['Drama']</td>\n",
       "      <td>7.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2582</th>\n",
       "      <td>1990</td>\n",
       "      <td>Narrow Margin</td>\n",
       "      <td>in los angeles ,  the editor of a publishing h...</td>\n",
       "      <td>['Action', 'Crime', 'Thriller']</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year                      title  \\\n",
       "3107  2003                       Most   \n",
       "900   2008  How to Be a Serial Killer   \n",
       "6724  1941             A Woman's Face   \n",
       "4704  1954            Executive Suite   \n",
       "2582  1990              Narrow Margin   \n",
       "\n",
       "                                                   plot  \\\n",
       "3107  most is the story of a single father who takes...   \n",
       "900   a serial killer decides to teach the secrets o...   \n",
       "6724  in sweden ,  a female blackmailer with a disfi...   \n",
       "4704  in a friday afternoon in new york ,  the presi...   \n",
       "2582  in los angeles ,  the editor of a publishing h...   \n",
       "\n",
       "                                  genres  rating  \n",
       "3107                  ['Short', 'Drama']     8.0  \n",
       "900        ['Comedy', 'Crime', 'Horror']     5.6  \n",
       "6724  ['Drama', 'Film-Noir', 'Thriller']     7.2  \n",
       "4704                           ['Drama']     7.4  \n",
       "2582     ['Action', 'Crime', 'Thriller']     6.6  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Importación librerías\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "# Carga de datos de archivo .csv\n",
    "dataTraining = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTesting.zip', encoding='UTF-8', index_col=0)\n",
    "\n",
    "# Visualización datos de entrenamiento\n",
    "dataTraining.head()\n",
    "\n",
    "# Visualización datos de test\n",
    "# dataTesting.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos os gêneros únicos na coluna 'genres':\n",
      "{'Documentary', 'Crime', 'Sport', 'Romance', 'Music', 'Fantasy', 'Musical', 'Biography', 'Family', 'Comedy', 'News', 'Short', 'Mystery', 'Horror', 'Film-Noir', 'Adventure', 'Drama', 'Western', 'Thriller', 'War', 'Animation', 'History', 'Action', 'Sci-Fi'}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregar os dados (substitua pelo caminho real do seu arquivo)\n",
    "dataTraining = pd.read_csv('https://github.com/albahnsen/MIAD_ML_and_NLP/raw/main/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)\n",
    "\n",
    "# Função para obter todos os gêneros únicos\n",
    "def obter_generos_unicos(df, coluna):\n",
    "    generos_unicos = set()\n",
    "    for generos in df[coluna]:\n",
    "        generos_lista = eval(generos)  # Converte a string da lista de volta para uma lista\n",
    "        for genero in generos_lista:\n",
    "            generos_unicos.add(genero)\n",
    "    return generos_unicos\n",
    "\n",
    "# Obter todos os gêneros únicos\n",
    "todos_generos = obter_generos_unicos(dataTraining, 'genres')\n",
    "print(\"Todos os gêneros únicos na coluna 'genres':\")\n",
    "print(todos_generos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPROCESAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/felipe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/felipe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/felipe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/felipe/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#mejor:PREPROCESAMIENTO3:n-gramas,TF-IDF Vectorization Y Lematización\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Preprocesamiento de Texto\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convertir todo a minúsculas\n",
    "    text = text.lower()\n",
    "    # Tokenizar el texto\n",
    "    tokens = word_tokenize(text)\n",
    "    # Eliminar stopwords, puntuaciones y números\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation and not word.isdigit()]\n",
    "    # Lematización\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # Unir los tokens nuevamente en un string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Aplicar preprocesamiento al texto\n",
    "dataTraining['plot_processed'] = dataTraining['plot'].apply(preprocess_text)\n",
    "\n",
    "# Vectorización del Texto usando TF-IDF con n-gramas\n",
    "#vectorizer = TfidfVectorizer(max_features=7800) #mejor modelo regresion logistica 0.88, SVR\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2)) #para el stacking,redes 0.86\n",
    "\n",
    "X = vectorizer.fit_transform(dataTraining['plot_processed'])\n",
    "y = dataTraining['genres']\n",
    "\n",
    "# Binarizar la variable objetivo\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(y.apply(eval))\n",
    "\n",
    "# División de los datos en entrenamiento y prueba\n",
    "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear tfidf_vectorizer.pkl\n",
    "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mlb.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(mlb, 'mlb.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELADO CON PREPROCESAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8255629781604181"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MODELO PROFE\n",
    "#Definición y entrenamiento\n",
    "clf4 = OneVsRestClassifier(RandomForestClassifier(n_jobs=-1, n_estimators=100, max_depth=10, random_state=42))\n",
    "clf4.fit(X_train4, y_train4)\n",
    "# Predicción del modelo de clasificación\n",
    "y_pred_rf4= clf4.predict_proba(X_test4)\n",
    "# Impresión del desempeño del modelo\n",
    "roc_auc_score(y_test4, y_pred_rf4, average='macro')\n",
    "#0.8255629781604181"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8859398196321558"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MEJOR:Regresión Logística Multiclase\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Crear el modelo de regresión logística multiclase\n",
    "logistic_reg4 = OneVsRestClassifier(LogisticRegression(max_iter=1000, random_state=42))\n",
    "logistic_reg4.fit(X_train4, y_train4)\n",
    "y_pred_logis4 = logistic_reg4.predict_proba(X_test4)\n",
    "roc_auc_score(y_test4, y_pred_logis4, average='macro')\n",
    "#0.8891961604881898"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.522705170276073"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb_classifier = OneVsRestClassifier(MultinomialNB())\n",
    "nb_classifier.fit(X_train4, y_train4)\n",
    "y_pred_NB2 = nb_classifier.predict(X_test4)\n",
    "roc_auc_score(y_test4, y_pred_NB2, average='macro')\n",
    "#0.522705170276073"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1039, number of negative: 5277\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.098004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.164503 -> initscore=-1.625099\n",
      "[LightGBM] [Info] Start training from score -1.625099\n",
      "[LightGBM] [Info] Number of positive: 816, number of negative: 5500\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.193805 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.129196 -> initscore=-1.908089\n",
      "[LightGBM] [Info] Start training from score -1.908089\n",
      "[LightGBM] [Info] Number of positive: 200, number of negative: 6116\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.079269 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.031666 -> initscore=-3.420346\n",
      "[LightGBM] [Info] Start training from score -3.420346\n",
      "[LightGBM] [Info] Number of positive: 290, number of negative: 6026\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.065126 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.045915 -> initscore=-3.033958\n",
      "[LightGBM] [Info] Start training from score -3.033958\n",
      "[LightGBM] [Info] Number of positive: 2429, number of negative: 3887\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.084129 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.384579 -> initscore=-0.470158\n",
      "[LightGBM] [Info] Start training from score -0.470158\n",
      "[LightGBM] [Info] Number of positive: 1161, number of negative: 5155\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095361 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.183819 -> initscore=-1.490685\n",
      "[LightGBM] [Info] Start training from score -1.490685\n",
      "[LightGBM] [Info] Number of positive: 336, number of negative: 5980\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.095456 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.053198 -> initscore=-2.879065\n",
      "[LightGBM] [Info] Start training from score -2.879065\n",
      "[LightGBM] [Info] Number of positive: 3182, number of negative: 3134\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.064934 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.503800 -> initscore=0.015200\n",
      "[LightGBM] [Info] Start training from score 0.015200\n",
      "[LightGBM] [Info] Number of positive: 523, number of negative: 5793\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073477 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.082806 -> initscore=-2.404824\n",
      "[LightGBM] [Info] Start training from score -2.404824\n",
      "[LightGBM] [Info] Number of positive: 563, number of negative: 5753\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.061004 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.089139 -> initscore=-2.324197\n",
      "[LightGBM] [Info] Start training from score -2.324197\n",
      "[LightGBM] [Info] Number of positive: 129, number of negative: 6187\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.071188 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.020424 -> initscore=-3.870393\n",
      "[LightGBM] [Info] Start training from score -3.870393\n",
      "[LightGBM] [Info] Number of positive: 227, number of negative: 6089\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.082011 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035940 -> initscore=-3.289289\n",
      "[LightGBM] [Info] Start training from score -3.289289\n",
      "[LightGBM] [Info] Number of positive: 785, number of negative: 5531\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.080350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.124288 -> initscore=-1.952440\n",
      "[LightGBM] [Info] Start training from score -1.952440\n",
      "[LightGBM] [Info] Number of positive: 265, number of negative: 6051\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.073592 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.041957 -> initscore=-3.128249\n",
      "[LightGBM] [Info] Start training from score -3.128249\n",
      "[LightGBM] [Info] Number of positive: 214, number of negative: 6102\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.060597 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.033882 -> initscore=-3.350396\n",
      "[LightGBM] [Info] Start training from score -3.350396\n",
      "[LightGBM] [Info] Number of positive: 614, number of negative: 5702\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.103516 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.097213 -> initscore=-2.228577\n",
      "[LightGBM] [Info] Start training from score -2.228577\n",
      "[LightGBM] [Info] Number of positive: 6, number of negative: 6310\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.071496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000950 -> initscore=-6.958131\n",
      "[LightGBM] [Info] Start training from score -6.958131\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 1503, number of negative: 4813\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.082386 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.237967 -> initscore=-1.163857\n",
      "[LightGBM] [Info] Start training from score -1.163857\n",
      "[LightGBM] [Info] Number of positive: 565, number of negative: 5751\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.064770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.089455 -> initscore=-2.320303\n",
      "[LightGBM] [Info] Start training from score -2.320303\n",
      "[LightGBM] [Info] Number of positive: 74, number of negative: 6242\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.062167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.011716 -> initscore=-4.434991\n",
      "[LightGBM] [Info] Start training from score -4.434991\n",
      "[LightGBM] [Info] Number of positive: 207, number of negative: 6109\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058378 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.032774 -> initscore=-3.384800\n",
      "[LightGBM] [Info] Start training from score -3.384800\n",
      "[LightGBM] [Info] Number of positive: 1630, number of negative: 4686\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.052855 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.258075 -> initscore=-1.055999\n",
      "[LightGBM] [Info] Start training from score -1.055999\n",
      "[LightGBM] [Info] Number of positive: 283, number of negative: 6033\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.055454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.044807 -> initscore=-3.059553\n",
      "[LightGBM] [Info] Start training from score -3.059553\n",
      "[LightGBM] [Info] Number of positive: 178, number of negative: 6138\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 90340\n",
      "[LightGBM] [Info] Number of data points in the train set: 6316, number of used features: 3132\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.028182 -> initscore=-3.540471\n",
      "[LightGBM] [Info] Start training from score -3.540471\n",
      "AUC Score: 0.8505648915346008\n"
     ]
    }
   ],
   "source": [
    "#LightGBM\n",
    "import lightgbm as lgb\n",
    "lgbm = lgb.LGBMClassifier(random_state=42,n_jobs=-1)\n",
    "classifier = OneVsRestClassifier(lgbm)\n",
    "classifier.fit(X_train4, y_train4)\n",
    "y_pred_ligth = classifier.predict_proba(X_test4)\n",
    "auc_score = roc_auc_score(y_test4, y_pred_ligth, average='macro')\n",
    "print(\"AUC Score:\", auc_score)\n",
    "#AUC Score: 0.8505648915346008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CatBoost nunca termino\n",
    "from catboost import CatBoostClassifier\n",
    "catboost_model = CatBoostClassifier(verbose=0, random_seed=42)\n",
    "ovr_catboost = OneVsRestClassifier(catboost_model)\n",
    "ovr_catboost.fit(X_train4, y_train4)\n",
    "y_pred_catbost = ovr_catboost.predict_proba(X_test4)\n",
    "auc_score = roc_auc_score(y_test4, y_pred_catbost, average='macro')\n",
    "print(\"AUC Score:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Mejores parámetros encontrados: {'estimator__C': 0.001}\n",
      "Mejor puntaje (ROC AUC): nan\n",
      "AUC: 0.8825829497048616\n"
     ]
    }
   ],
   "source": [
    "#CALIBRACION#1 con RandomizedSearchCV no se uso gride por lo pesado que puede ser \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Crear el modelo base de regresión logística\n",
    "base_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Crear el clasificador OneVsRest con el modelo base\n",
    "classifier = OneVsRestClassifier(base_model)\n",
    "\n",
    "# Definir los parámetros a buscar\n",
    "param_dist = {'estimator__C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "random_search = RandomizedSearchCV(estimator=classifier,\n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=10,\n",
    "                                   scoring='roc_auc',\n",
    "                                   cv=5,\n",
    "                                   verbose=2,\n",
    "                                   n_jobs=-1)\n",
    "\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros\n",
    "random_search.fit(X_train4, y_train4)\n",
    "\n",
    "# Obtener el mejor modelo y sus hiperparámetros\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Imprimir los mejores parámetros y el mejor puntaje\n",
    "print(\"Mejores parámetros encontrados:\", best_params)\n",
    "print(\"Mejor puntaje (ROC AUC):\", random_search.best_score_)\n",
    "\n",
    "\n",
    "# Entrenar el modelo con los datos de entrenamiento\n",
    "best_model.fit(X_train4, y_train4)\n",
    "\n",
    "# Predecir las probabilidades en el conjunto de prueba\n",
    "y_pred_proba = best_model.predict_proba(X_test4)\n",
    "\n",
    "# Calcular el AUC\n",
    "auc = roc_auc_score(y_test4, y_pred_proba, average='macro')\n",
    "print(\"AUC:\", auc)\n",
    "#AUC-ROC Score: 0.8580425171177647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score del Stacking Classifier: 0.8796844834549841\n"
     ]
    }
   ],
   "source": [
    "#STACKING\n",
    "# Definir modelos base\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "# Definición de los modelos base\n",
    "model1 = ('logistic_reg', LogisticRegression(max_iter=1000, random_state=42))\n",
    "model3 = ('random_forest', RandomForestClassifier(n_jobs=-1, n_estimators=50, max_depth=10, random_state=42))\n",
    "\n",
    "# Definición del meta-modelo\n",
    "meta_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Definición del Stacking Classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[model1, model3],\n",
    "    final_estimator=meta_model,\n",
    "    cv=3,  \n",
    "    n_jobs=1)\n",
    "\n",
    "# Usar MultiOutputClassifier para manejar múltiples salidas\n",
    "multi_output_stacking = MultiOutputClassifier(stacking_model, n_jobs=1)\n",
    "\n",
    "# Entrenamiento del Stacking Classifier\n",
    "multi_output_stacking.fit(X_train4, y_train4)\n",
    "\n",
    "# Predicción del modelo de ensamblaje\n",
    "y_pred_stacking = multi_output_stacking.predict_proba(X_test4)\n",
    "\n",
    "# Evaluación del desempeño del modelo\n",
    "auc_score = roc_auc_score(y_test4, np.hstack([y[:, 1][:, np.newaxis] for y in y_pred_stacking]), average='macro')\n",
    "print(\"AUC Score del Stacking Classifier:\", auc_score)\n",
    "#0.8796844834549841"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STACKING2\n",
    "# Definir modelos base\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "\n",
    "# Definición de los modelos base\n",
    "model1 = ('logistic_reg', LogisticRegression(max_iter=1000, random_state=42))\n",
    "model2=('SV',SVC(kernel='linear', probability=True, random_state=42))\n",
    "# Definición del meta-modelo\n",
    "meta_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Definición del Stacking Classifier\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=[model1,model2],\n",
    "    final_estimator=meta_model,\n",
    "    cv=3,  \n",
    "    n_jobs=1)\n",
    "\n",
    "# Usar MultiOutputClassifier para manejar múltiples salidas\n",
    "multi_output_stacking2 = MultiOutputClassifier(stacking_model, n_jobs=1)\n",
    "\n",
    "# Entrenamiento del Stacking Classifier\n",
    "multi_output_stacking2.fit(X_train4, y_train4)\n",
    "\n",
    "# Predicción del modelo de ensamblaje\n",
    "y_pred_stacking = multi_output_stacking2.predict_proba(X_test4)\n",
    "\n",
    "# Evaluación del desempeño del modelo\n",
    "auc_score = roc_auc_score(y_test4, np.hstack([y[:, 1][:, np.newaxis] for y in y_pred_stacking]), average='macro')\n",
    "print(\"AUC Score del Stacking Classifier:\", auc_score)\n",
    "#AUC Score del Stacking Classifier: 0.8485437202542346"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Mejores parámetros encontrados: {'model__neurons': 256, 'model__learning_rate': 0.001, 'model__dropout_rate': 0.5, 'epochs': 10, 'batch_size': 32}\n",
      "Mejor puntaje (ROC AUC): 0.8687475581760081\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Definir una función para crear el modelo\n",
    "def create_model(neurons=128, learning_rate=0.001, dropout_rate=0.5):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim=X_train4.shape[1], activation='relu'))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(y_train4.shape[1], activation='sigmoid'))\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "# Crear el KerasClassifier usando scikeras\n",
    "model = KerasClassifier(model=create_model, verbose=0)\n",
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_dist = {\n",
    "    'model__neurons': [64, 128, 256],\n",
    "    'model__learning_rate': [0.001, 0.01, 0.1],\n",
    "    'model__dropout_rate': [0.3, 0.5, 0.7],\n",
    "    'batch_size': [32, 64, 128],\n",
    "    'epochs': [10, 20]\n",
    "}\n",
    "# Realizar la búsqueda de hiperparámetros\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, scoring='roc_auc', cv=3, verbose=2, n_jobs=-1)\n",
    "# Entrenar el modelo usando RandomizedSearchCV\n",
    "random_search.fit(X_train4.toarray(), y_train4)\n",
    "# Obtener el mejor modelo y sus hiperparámetros\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "# Imprimir los mejores parámetros y el mejor puntaje\n",
    "print(\"Mejores parámetros encontrados:\", best_params)\n",
    "print(\"Mejor puntaje (ROC AUC):\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "Mejores parámetros encontrados: {'batch_size': 32, 'epochs': 10, 'model__dropout_rate': 0.3, 'model__learning_rate': 0.001, 'model__neurons': 128}\n",
      "Mejor puntaje (ROC AUC): 0.8641089465419985\n",
      "AUC Score del mejor modelo de red neuronal: 0.6230654726763062\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir el espacio de búsqueda de hiperparámetros (menos combinaciones para aligerar el proceso)\n",
    "param_grid = {\n",
    "    'model__neurons': [64, 128],\n",
    "    'model__learning_rate': [0.001, 0.01],\n",
    "    'model__dropout_rate': [0.3, 0.5],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [10, 20]\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='roc_auc', cv=3, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Entrenar el modelo usando GridSearchCV\n",
    "grid_search.fit(X_train4.toarray(), y_train4)\n",
    "\n",
    "# Obtener el mejor modelo y sus hiperparámetros\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Imprimir los mejores parámetros y el mejor puntaje\n",
    "print(\"Mejores parámetros encontrados:\", best_params)\n",
    "print(\"Mejor puntaje (ROC AUC):\", grid_search.best_score_)\n",
    "\n",
    "# Evaluación del mejor modelo en el conjunto de prueba\n",
    "y_pred_RED1_best = best_model.predict(X_test4.toarray())\n",
    "auc_score = roc_auc_score(y_test4, y_pred_RED1_best, average='macro')\n",
    "print(\"AUC Score del mejor modelo de red neuronal:\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "79/79 [==============================] - 2s 21ms/step - loss: 1.1093 - accuracy: 0.1663 - val_loss: 0.8154 - val_accuracy: 0.2104\n",
      "Epoch 2/50\n",
      "79/79 [==============================] - 1s 18ms/step - loss: 0.6775 - accuracy: 0.1997 - val_loss: 0.5682 - val_accuracy: 0.2104\n",
      "Epoch 3/50\n",
      "79/79 [==============================] - 2s 23ms/step - loss: 0.5243 - accuracy: 0.2031 - val_loss: 0.4626 - val_accuracy: 0.2104\n",
      "Epoch 4/50\n",
      "79/79 [==============================] - 2s 19ms/step - loss: 0.4391 - accuracy: 0.2065 - val_loss: 0.3982 - val_accuracy: 0.2104\n",
      "Epoch 5/50\n",
      "79/79 [==============================] - 2s 20ms/step - loss: 0.3852 - accuracy: 0.2043 - val_loss: 0.3586 - val_accuracy: 0.2104\n",
      "Epoch 6/50\n",
      "79/79 [==============================] - 2s 23ms/step - loss: 0.3531 - accuracy: 0.2057 - val_loss: 0.3341 - val_accuracy: 0.2104\n",
      "Epoch 7/50\n",
      "79/79 [==============================] - 2s 21ms/step - loss: 0.3323 - accuracy: 0.2126 - val_loss: 0.3187 - val_accuracy: 0.2104\n",
      "Epoch 8/50\n",
      "79/79 [==============================] - 2s 21ms/step - loss: 0.3190 - accuracy: 0.2053 - val_loss: 0.3092 - val_accuracy: 0.2104\n",
      "Epoch 9/50\n",
      "79/79 [==============================] - 2s 20ms/step - loss: 0.3113 - accuracy: 0.2096 - val_loss: 0.3033 - val_accuracy: 0.2104\n",
      "Epoch 10/50\n",
      "79/79 [==============================] - 2s 23ms/step - loss: 0.3059 - accuracy: 0.2078 - val_loss: 0.2996 - val_accuracy: 0.2104\n",
      "Epoch 11/50\n",
      "79/79 [==============================] - 2s 20ms/step - loss: 0.3030 - accuracy: 0.2035 - val_loss: 0.2973 - val_accuracy: 0.2104\n",
      "Epoch 12/50\n",
      "79/79 [==============================] - 2s 25ms/step - loss: 0.3011 - accuracy: 0.2049 - val_loss: 0.2957 - val_accuracy: 0.2104\n",
      "Epoch 13/50\n",
      "79/79 [==============================] - 2s 21ms/step - loss: 0.2995 - accuracy: 0.2041 - val_loss: 0.2950 - val_accuracy: 0.2104\n",
      "Epoch 14/50\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.2987 - accuracy: 0.2019 - val_loss: 0.2941 - val_accuracy: 0.2104\n",
      "Epoch 15/50\n",
      "79/79 [==============================] - 2s 24ms/step - loss: 0.2983 - accuracy: 0.2003 - val_loss: 0.2938 - val_accuracy: 0.2104\n",
      "Epoch 16/50\n",
      "79/79 [==============================] - 2s 20ms/step - loss: 0.2976 - accuracy: 0.1973 - val_loss: 0.2935 - val_accuracy: 0.2104\n",
      "Epoch 17/50\n",
      "79/79 [==============================] - 2s 21ms/step - loss: 0.2971 - accuracy: 0.2017 - val_loss: 0.2932 - val_accuracy: 0.2104\n",
      "Epoch 18/50\n",
      "79/79 [==============================] - 2s 20ms/step - loss: 0.2967 - accuracy: 0.1985 - val_loss: 0.2931 - val_accuracy: 0.2104\n",
      "Epoch 19/50\n",
      "79/79 [==============================] - 2s 22ms/step - loss: 0.2964 - accuracy: 0.2003 - val_loss: 0.2931 - val_accuracy: 0.2104\n",
      "Epoch 20/50\n",
      "79/79 [==============================] - 2s 20ms/step - loss: 0.2968 - accuracy: 0.1979 - val_loss: 0.2929 - val_accuracy: 0.2104\n",
      "Epoch 21/50\n",
      "79/79 [==============================] - 2s 24ms/step - loss: 0.2962 - accuracy: 0.1983 - val_loss: 0.2929 - val_accuracy: 0.2104\n",
      "Epoch 22/50\n",
      "79/79 [==============================] - 2s 23ms/step - loss: 0.2961 - accuracy: 0.1989 - val_loss: 0.2930 - val_accuracy: 0.2104\n",
      "Epoch 23/50\n",
      "79/79 [==============================] - 2s 22ms/step - loss: 0.2964 - accuracy: 0.1995 - val_loss: 0.2927 - val_accuracy: 0.2104\n",
      "Epoch 24/50\n",
      "79/79 [==============================] - 2s 27ms/step - loss: 0.2961 - accuracy: 0.1983 - val_loss: 0.2928 - val_accuracy: 0.2104\n",
      "Epoch 25/50\n",
      "79/79 [==============================] - 2s 30ms/step - loss: 0.2953 - accuracy: 0.1981 - val_loss: 0.2927 - val_accuracy: 0.2104\n",
      "Epoch 26/50\n",
      "79/79 [==============================] - 2s 23ms/step - loss: 0.2955 - accuracy: 0.1985 - val_loss: 0.2926 - val_accuracy: 0.2104\n",
      "Epoch 27/50\n",
      "79/79 [==============================] - 2s 26ms/step - loss: 0.2959 - accuracy: 0.1979 - val_loss: 0.2926 - val_accuracy: 0.2104\n",
      "Epoch 28/50\n",
      "79/79 [==============================] - 2s 25ms/step - loss: 0.2955 - accuracy: 0.1989 - val_loss: 0.2926 - val_accuracy: 0.2104\n",
      "Epoch 29/50\n",
      "79/79 [==============================] - 2s 25ms/step - loss: 0.2950 - accuracy: 0.1983 - val_loss: 0.2926 - val_accuracy: 0.2104\n",
      "Epoch 30/50\n",
      "79/79 [==============================] - 2s 23ms/step - loss: 0.2951 - accuracy: 0.1975 - val_loss: 0.2925 - val_accuracy: 0.2104\n",
      "Epoch 31/50\n",
      "79/79 [==============================] - 2s 21ms/step - loss: 0.2950 - accuracy: 0.1979 - val_loss: 0.2926 - val_accuracy: 0.2104\n",
      "Epoch 32/50\n",
      "79/79 [==============================] - 2s 22ms/step - loss: 0.2952 - accuracy: 0.1981 - val_loss: 0.2925 - val_accuracy: 0.2104\n",
      "Epoch 33/50\n",
      "79/79 [==============================] - 2s 21ms/step - loss: 0.2952 - accuracy: 0.1979 - val_loss: 0.2926 - val_accuracy: 0.2104\n",
      "Epoch 34/50\n",
      "79/79 [==============================] - 2s 23ms/step - loss: 0.2947 - accuracy: 0.1979 - val_loss: 0.2924 - val_accuracy: 0.2104\n",
      "Epoch 35/50\n",
      "79/79 [==============================] - 2s 23ms/step - loss: 0.2950 - accuracy: 0.1975 - val_loss: 0.2925 - val_accuracy: 0.2104\n",
      "Epoch 36/50\n",
      "79/79 [==============================] - 2s 23ms/step - loss: 0.2951 - accuracy: 0.1981 - val_loss: 0.2925 - val_accuracy: 0.2104\n",
      "Epoch 37/50\n",
      "79/79 [==============================] - 2s 23ms/step - loss: 0.2947 - accuracy: 0.1981 - val_loss: 0.2925 - val_accuracy: 0.2104\n",
      "Epoch 38/50\n",
      "79/79 [==============================] - 2s 25ms/step - loss: 0.2945 - accuracy: 0.1979 - val_loss: 0.2925 - val_accuracy: 0.2104\n",
      "Epoch 39/50\n",
      "79/79 [==============================] - 2s 24ms/step - loss: 0.2948 - accuracy: 0.1979 - val_loss: 0.2925 - val_accuracy: 0.2104\n",
      "50/50 [==============================] - 0s 3ms/step\n",
      "AUC Score de la red neuronal: 0.611088377770746\n"
     ]
    }
   ],
   "source": [
    "#RED2: PARA ASEGURAR Q DE MEJOR QUE LA RED ORIGINAL \n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.regularizers import l2\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=X_train4.shape[1], activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y_train4.shape[1], activation='sigmoid'))\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer=RMSprop(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train4.toarray(), y_train4, epochs=50, batch_size=64, validation_split=0.2, verbose=1, callbacks=[early_stopping])\n",
    "# Evaluación del modelo\n",
    "y_pred_RED3 = model.predict(X_test4.toarray())\n",
    "auc_score = roc_auc_score(y_test4, y_pred_RED3, average='macro')\n",
    "print(\"AUC Score de la red neuronal:\", auc_score)\n",
    "#0.611088377770746"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción conjunto de test para envío a Kaggle\n",
    "En esta sección encontrarán el formato en el que deben guardar los resultados de la predicción para que puedan subirlos a la competencia en Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformación variables predictoras X del conjunto de test\n",
    "X_test_dtm = vectorizer.transform(dataTesting['plot'])\n",
    "\n",
    "cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
    "        'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
    "        'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
    "\n",
    "# Predicción del conjunto de test\n",
    "y_pred_test_genres = logistic_reg4.predict_proba(X_test_dtm)\n",
    "# Guardar predicciones en formato exigido en la competencia de kaggle\n",
    "res = pd.DataFrame(y_pred_test_genres, index=dataTesting.index, columns=cols)\n",
    "res.to_csv('pred_preprocesam5.csv', index_label='ID')\n",
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['modelo.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exportando modelo\n",
    "import sklearn\n",
    "import joblib\n",
    "print(sklearn.__version__)\n",
    "joblib.dump(logistic_reg4, 'modelo.pkl', protocol=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
