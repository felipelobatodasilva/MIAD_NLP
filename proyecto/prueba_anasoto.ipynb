{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399831, 6)\n",
      "(388972, 6)\n"
     ]
    }
   ],
   "source": [
    "# Importación librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, PolynomialFeatures\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV,cross_val_score\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# Carga de datos de archivo .csv\n",
    "dataTraining = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2023/main/datasets/dataTrain_carListings.zip')\n",
    "dataTesting = pd.read_csv('https://raw.githubusercontent.com/davidzarruk/MIAD_ML_NLP_2023/main/datasets/dataTest_carListings.zip', index_col=0)\n",
    "duplicados = dataTraining.duplicated().sum()\n",
    "df_train2 = dataTraining.drop_duplicates()\n",
    "Q1 = df_train2['Price'].quantile(0.25)\n",
    "Q3 = df_train2['Price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "limite_inferior = Q1 - 1.5 * IQR\n",
    "limite_superior = Q3 + 1.5 * IQR\n",
    "\n",
    "# Identificar valores atípicos del precio y quitandolos\n",
    "valores_atipicos = df_train2[(df_train2['Price'] < limite_inferior) | (df_train2['Price'] > limite_superior)]\n",
    "df_train3 = df_train2[(df_train2['Price'] >= limite_inferior) & (df_train2['Price'] <= limite_superior)]\n",
    "\n",
    "#Identificando valores atipicos de Mileage y quitandolos \n",
    "df_train3 = df_train3[df_train3['Mileage'] <= 1500000]\n",
    "print(df_train2.shape)\n",
    "print(df_train3.shape)\n",
    "\n",
    "# Supongamos que tienes tus datos en un DataFrame llamado df_train3\n",
    "\n",
    "# División de datos en características y variable objetivo\n",
    "X = df_train3.drop('Price', axis=1)\n",
    "y = df_train3['Price']\n",
    "\n",
    "#Ingenieria de caracteristicas\n",
    "current_year = datetime.now().year\n",
    "X['Car_Age'] = current_year - X['Year']\n",
    "X['Mileage_Year'] = X['Year'] / X['Mileage']\n",
    "X['Brand_Model'] = X['Make'] + '_' + X['Model']\n",
    "\n",
    "# Listas de características numéricas y categóricas\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Definición de transformadores para características numéricas y categóricas\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "# ColumnTransformer para aplicar transformadores a diferentes tipos de características\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Pipeline que incluye el preprocesamiento y el modelo\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor)])\n",
    "\n",
    "# Aplicar el pipeline a los datos\n",
    "X_preprocessed = pipeline.fit_transform(X)\n",
    "\n",
    "# Division de los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.33, random_state=42)\n",
    "#MeTRICA\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2998.231894311593\n"
     ]
    }
   ],
   "source": [
    "#XGBOOST\n",
    "xgboost_model = XGBRegressor(random_state=12345,subsample= 0.95, reg_lambda= 1.5, n_estimators= 500, max_depth= 7, learning_rate= 0.1, gamma= 0.3, colsample_bytree=0.8)\n",
    "xgboost_model.fit(X_train, y_train)\n",
    "y_pred_xgb = xgboost_model.predict(X_test)\n",
    "rmse = root_mean_squared_error(y_test, y_pred_xgb)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBOOST CON SELECCION DE CARACTERISTICAS IMPORTANTES, falta hacer el modelo\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Entrenar un modelo de Random Forest\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train, y_train)\n",
    "# Obtener la importancia de las características\n",
    "feature_importances = rf_model.feature_importances_\n",
    "# Crear un selector de características basado en la importancia de las características\n",
    "selector = SelectFromModel(rf_model, threshold='median')\n",
    "# Aplicar el selector a los datos de entrenamiento\n",
    "X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "# Aplicar el mismo selector a los datos de prueba\n",
    "X_test_selected = selector.transform(X_test)\n",
    "# Obtener las características seleccionadas\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "# Imprimir las características seleccionadas\n",
    "print(\"Características seleccionadas:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros encontrados: {'subsample': 0.7, 'reg_lambda': 2, 'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0.1, 'colsample_bytree': 0.7}\n",
      "RMSE en el conjunto de prueba con los mejores parámetros: 3818.467429996832\n"
     ]
    }
   ],
   "source": [
    "#BUSQUEDA DE HIPERPARAMETROS DE XGBOOST\n",
    "# Definir el modelo\n",
    "xgboost_model = XGBRegressor(random_state=12345)\n",
    "\n",
    "# Definir la cuadrado del error medio como métrica para optimizar\n",
    "scoring = make_scorer(mean_squared_error, squared=False)\n",
    "\n",
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_distributions = {\n",
    "    'n_estimators': [400, 500],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1,0.15, 0.2],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.8, 0.9, 0.95],\n",
    "    'reg_lambda': [1, 1.5, 2],\n",
    "    'gamma': [0, 0.1, 0.3],\n",
    "    'colsample_bytree':[0.7,0.8]\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda aleatoria de hiperparámetros\n",
    "random_search = RandomizedSearchCV(xgboost_model, param_distributions, n_iter=50, cv=5, scoring=scoring, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el mejor modelo y sus hiperparámetros\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Imprimir los mejores parámetros\n",
    "print(\"Mejores parámetros encontrados:\", best_params)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "y_pred_best = best_xgb_model.predict(X_test)\n",
    "rmse_best = root_mean_squared_error(y_test, y_pred_best)\n",
    "print(\"RMSE en el conjunto de prueba con los mejores parámetros:\", rmse_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20987.104 36943.902 15242.517  8612.789 30949.766]\n"
     ]
    }
   ],
   "source": [
    "#data de competencia\n",
    "dataTesting['Car_Age'] = current_year - dataTesting['Year']\n",
    "dataTesting['Mileage_Year'] = dataTesting['Year'] / dataTesting['Mileage']\n",
    "dataTesting['Brand_Model'] = dataTesting['Make'] + '_' + dataTesting['Model']\n",
    "\n",
    "X_testcru = pipeline.transform(dataTesting)\n",
    "y_predmodelo1 = xgboost_model.predict(X_testcru)\n",
    "print(y_predmodelo1[:5]) \n",
    "df = pd.DataFrame(y_predmodelo1) \n",
    "# Rename the default column (likely '0') to 'Price'\n",
    "df = df.rename(columns={0: 'Price'}) \n",
    "# Save to CSV \n",
    "df.to_csv(r\"C:\\Users\\1234\\Downloads\\MIAD_NLP\\proyecto\\modeloxgboost8.csv\", index_label='ID') \n",
    "#4144 en kaggle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
